# tiny_rpc

哎，很多东西在面试官的面前说的话，非常急，就是可能条例不是很清晰，所以将我的话写在了下面

## 项目流程

### 网络相关

- 为什么Tcp是三次握手，而不是其他次握手？
  - 哎，元戎二面提了这个，我没有很好的回答出来
  - 原因：
    - 阻止重复历史连接的初始化（主要原因）（清除历史记录）
    - 同步对方的初始序列号
    - 避免资源的浪费
  - 四次握手当然可以解决这个问题，但是没有必要
- time_wait状态一般发生在哪一个方向？
  - 客户端/服务端都可以出现，但是一般是发生在服务端，因为服务端的端口和ip地址都是固定的，但是客户端的端口和ip地址是不固定的，所以说一般是服务端是time_wait状态
- 什么是SYN攻击？
  - 当客户端只是发送SYN，但是并不没有
- OS kernel相关？
  - OS会维护两个队列，Tcp半连接状态和Tcp全连接状态。
    - 如果说全连接状态非空，就会给accept()返回一个文件描述符
- `bind("0.0.0.0")`这样的地址，代表的是什么？
  - 服务端才会bind()，代表的是监听所有的的地址，所有的ip地址都会监听。

### protobuf相关

- 为什么使用protobuf这个序列化工具的呢？
  - 因为序列化非常简单，编码出来的大小比xml，json小很多，因为可以节省带宽，利用了一些编码方式对这个数据的序列化，产生更好的数据
- protobuf中各个类的作用是什么？
  - 要继承这些类，重写里面的 virtual函数，才能使用
  - `google::protobuf::Message`：进行反序列化的信息和序列话的信息
  - `google::protobuf::Service`: `CallMethod()`,执行对应的函数
  - `google::protobuf::controller`：RpcController，相当于一个配置文件
  - `google::protobuf::Closure`: 执行的回调函数, 好像并没有用上

### 客户端/服务端

- 因为`Everything is file`，网络的来的是accept()的来的文件描述符，所以说EventLoop·
- 服务器主要的是`TcpServer`这个类，当创建这个类的的时候，就会创建`mainReactor`和`subReactor`，创建的端口要会一直监听，注意：这里的当创建的时候，mainReactor，就会开始loop循环了，subReactor就已经开始loop循环了，`这里使用了信号量和互斥锁的性质去保护这个线程`
  - 因为在执行到构造函数的时候，就会开辟线程，要等到一些对象初始化完之后，你才能够使用开辟的线程去处理任务，这里使用了`信号量`的机制去控制这个线程的进行
- 在Linux中一切都是文件，`Everything is file`，因为最终会将数据放到`epoll`中进行处理，那么这里要处理函数怎么办？其实就是`epoll_event`,`epoll_event.data.ptr`是一个`void*`，将FdEvent绑定到这个上面，获取获取到`Fdevent`,获取其中的`函数`，然后加入到本地的pending任务中，然后最后执行就可以了。
  - 函数判断的时候其实跟位运算的差不多似的,（使用的是位掩码）所以这里实际上应该是`trigger_event.events & EPOLLIN`，如果说要取消，那么大致是`m_listen_events.events &= (~EPOLLIN);`，这样取反就可以了
- `TcpConnection`代表是的是客户端与服务端的一条连接，一般服务端是固定的，那么当一条客户端与服务端连接成功了，那么就有有一条`TcpConnection`的连接，注意：`TcpConnection`中，会有一个`EventLoop`对象，相当于一个绑定，`一个TcpConnection中随机得到一个EventLoop`，然后将处理事件加入到`EventLoop`中。
- 客户端发送的时候发送的直接是`AbstraceProtocol`这样的类，然后将其

## 面试的

- IO多路复用是什么？
  - 以前处理请求的时候，客户端可以开一个进程，或者说开一个线程去处理他，但是这样也太浪费时间了，因为开辟进程和线程的时间消费是非常高的，（上下文切换啥的，两毫秒那样的时间复杂度），于是就产生了IO多路复用。使用一个进程来维护多个socket()，这样就可以在一个进程中处理很多的IO事件
- select/poll/epoll的相关考点
  - select
    - fd集合是一个数组，每次使用都会将其从用户态拷贝到内核态，耗费时间
    - 数组需要便利，浪费时间
    - 文件描述符是1024,不够大
  - poll
    - poll其实就是突破了文件描述符的最大限制,并且固定大小的集合换成了由链表实现的动态数组。但是还是受文件描述符的限制。
  - epoll
    - 将用户关心的文件描述符事件存放在内核中，通过内存映射，在用户态进行访问，（重复拷贝是利用mmap这样的技术来实现避免拷贝
    - 将事件（回调函数）加入到就绪列表中，调用`epoll_wait`就可以检查是否有多于的fd,然后给用户就可以了
    - 工作模式：
      - 水平触发（默认）：当有刻度事件的时候，就会一直epoll_wait()等待数据的出现，直到内核缓冲区数据被read()完之后才能使用。
      - 边缘触发：当有可读事件触发的时候，就会从epoll_wait中只wait()一次，告诉有数据来

### 工作模式

- LT模式：默认的工作模式，检测触发的时候，不会立刻处理事件，放到就绪列表中，下次使用，再通知
- ET模式：必须要处理，下次调用的时候，不会再次相应（只支持非堵塞模式），

## 项目疑问

- 主要实现的功能？
  - 一个是
- IO多路复用于线程池的池化思想有什么共同和特殊之处？为什么不用池化的思想去处理数据
  - 普通的线程池不能这样处理，因为客户端的请求可能会来很多这样的请求，比如说成百上千个，池化的思想虽然说里面有线程池，但是远远不够这些数字，所以说只能用IO多路复用，`IO多路复用一次性可以处理成百上千的任务，但是线程池可执行不了这些任务，会堵塞在那里`。
- Muti-Reactor的架构的优势是什么？为什么选用这个架构？
  - 传统的线程模型都是堵塞的类型，也就是说，当一个线程去处理一个IO事件的时候，就会堵塞在这里，发生堵塞的话，就没有办法处理其他的事务了，那么多个任务来到这里时候，就会堵在这里，没有更好的办法处理任务，
  - 那么对于Reactor架构呢，他是非堵塞的，主线程是一个EvnetLoop循环，还有四个副线程，也就是subReactor，当有任务监听的时候，主线程将这个文件描述符`Dispatcher`一下，将这些文件描述符给subReactor进行处理，所以说主线程是非堵塞的，异步的，然后subReactor处理之后，将结果发送给客户端，除了客户端需要等待这个结果，其他的都是异步执行的。
- 项目中实现了什么基本内容？
  - 是一个非常简易的Reactor架构的rpc框架，好像确实比较简单。。主要实现的基本内容有：
    - Reactor架构中十分重要的主从EventLoop的IO多路复用循环体系结构
    - 为了在网络上进行传输，实现了自定义协议包，然后使用protobuf进行序列化
    - 实现了一个简单的异步日志进行打印日志的功能99999999999999999999999999999999
- 日志的处理？竞争状态？
  - 每一个线程中会将日志输送到一个共同的buffer里面，这时候因为涉及到竞争状态，所以就需要互斥锁去锁住这个资源，防止出现数据竞争
  - 之后，会将一个定时任务推送到EventLoop循环中进行执行，这时候，每隔一段事件去判断判断将这个logger里面的数据输送到AsyncLogger的loop循环中，然后将日志打印到本地，方便调试。这里相当于一个`生产者-消费者的模型`。
- 为什么要设计自定义协议？为什么采用protobuf这样的序列化的东西？可以不序列化后传输到网络上，然后直接读取么？
  - 不行的，因为 网络序和字节序是不一样的，网络序是大端，然后字节序是小端，你如果直接传输，会相反，所以要进行序列化，然后变成网络序，然后再变成字节序
  - 网络上传输的都是字节，但是本地的是得到的结果是字符串，那么你就需要序列化成字节，然后再在网络上进行传播，
  - Tcp设置协议的时候不会说有粘包这一说，因为字节流，Tcp只是保证了你会传送到这个端口上，但是并不保证了数据是怎么字节的划分，这时候就需要设计一个自定义协议，提供给客户端和服务端进行接卸
- 协议的大致样子是什么呢？
  - 开始符 - 整包长度 - MsigID长度 - MsgID - 方法名长度 - 方法名 - 错误码 - 错误信息长度 - 错误信息 - protobuf序列化数据 - 校验码 - 结束符
  - 设计校验码的问题是：`Tcp只是保证了数据按照顺序到达你这个服务端，但是并没有保证数据可能出现泄密篡改，所以说还要进行安全校验`。
- 协议中为什么会设置一个`msg_id`这样的序列号？
  - 防止串包：当客户端的一个端口号有多个请求的时候，因为你要进行远程过程调用，那么你就要出现串包（请求是1 2 发送，可能收到的时候是 2 1 这样收到，就会发生串包），所以这个时候就要设计一个msg_id，保证这个序列化号，而且要唯一
- 协议是如何进行可扩展的？
  - 协议是设计成虚函数的样子进行扩展的，最上面的是函数是虚函数，然后下面是实现的函数，`注意：这里的有一个专门的智能指针进行转换`
- 这个项目中什么是非堵塞的，什么是堵塞的？
  - 项目中，有一个主线程，就是mainReactor，他是非杜塞的，对于日志，当有数据的时候，主线程不会阻塞，会将处理这个请求给subReactor，交给subReactor，有点想线程池的概念，在subReactor里面执行，但是subReacor请求任务时候，这时候并不是异步的，而不是同步的，因为他要进行函数处理。
  - 这里申请的文件描述符是需要需要申请非堵塞：因为客户端当有多个connect连接的时候，如果有一连接都断开的客户端，那么服务端accept就会堵塞，这时候就会一直杜塞到这里，所以说要设置成非堵塞。
- 函数处理的时候，统一使用的是`std::function<void()>`，虽然说`std::function<void()>`有性能损耗，但是他使用起来比较方便。
- 设置非堵塞的地方：
  - `setsockopt(m_listenfd, SOL_SOCKET, SO_REUSEPORT, &valid, sizeof(valid));`：减少非堵塞的情况。`处于time_wait`状态的不能用，这里要这样处理。
- 如何优雅的关闭
  - Tcp连接建立的时候，其实是建立两个相互不干扰的流的，输入和输出流，如果说直接调用`close(fd)`，那么就会同时关闭这两条流。
  - 因为虽然输出输出完了，但是可能她还没接受完，所以说你就需要使用`shutdown()`关闭一方的流，否则数据的传输可能出现问题。
- 为什么采用Reactor架构，这个架构有什么好用的地方？
  - 以前的业务处理，从网络中read()相关的信息，然后将其业务处理，然后发送给对方，如果网络中没有数据可读，那么就会进行堵塞，处理多个任务的时候，就会堵塞
  - IO多路复用可以解决这样的问题，当有事件可读的时候，就会epoll_wait返回可读的事件，这里实现的是多Reactor多线程的模式，每一个主Reactor就是一个线程，

## 项目代码构建

- 项目的基本流程：
  - 从 buffer里面读取数据，然后解码得到对应的protobuf对象，然后解析相关的内容
  - 找到对应的`request type` 和 `response type` 请求类型和返回的类型
  - 执行相关的函数
  - 将reponse对象序列化成pb_data，然后将其塞入到结构体中，然后将其encode,序列化之后发送到客户端手中
- 项目的大致流程
  - 客户端
  - 服务端

## 项目遇到的问题

- 