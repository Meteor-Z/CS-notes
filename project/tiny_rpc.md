# tiny_rpc

哎，很多东西在面试官的面前说的话，非常急，就是可能条例不是很清晰，所以将我的话写在了下面

## 项目流程

### 服务端

- 服务器主要的是`TcpServer`这个类，当创建这个类的的时候，就会创建`mainReactor`和`subReactor`，创建的端口要会一直监听，注意：这里的当创建的时候，mainReactor，就会开始loop循环了，subReactor就已经开始loop循环了，`这里使用了信号量和互斥锁的性质去保护这个线程`
  - 因为在执行到构造函数的时候，就会开辟线程，要等到一些对象初始化完之后，你才能够使用开辟的线程去处理任务，这里使用了`信号量`的机制去控制这个线程的进行
- 在Linux中一切都是文件，`Everything is file`，因为最终会将数据放到`epoll`中进行处理，那么这里要处理函数怎么办？其实就是`epoll_event`,`epoll_event.data.ptr`是一个`void*`，将FdEvent绑定到这个上面，获取获取到`Fdevent`,获取其中的`函数`，然后加入到本地的pending任务中，然后最后执行就可以了。
  - 函数判断的时候其实跟位运算的差不多似的,（使用的是位掩码）所以这里实际上应该是`trigger_event.events & EPOLLIN`，如果说要取消，那么大致是`m_listen_events.events &= (~EPOLLIN);`，这样取反就可以了
- 

## protobuf相关

- 为什么使用protobuf这个序列化工具的呢？
  - 因为序列化非常简单，编码出来的大小比xml，json小很多，因为可以节省带宽，利用了一些编码方式对这个数据的序列化，产生更好的数据
- protobuf中各个类的作用是什么？
  - 要继承这些类，重写里面的 virtual函数，才能使用
  - `google::protobuf::Message`：进行反序列化的信息和序列话的信息
  - `google::protobuf::Service`: `CallMethod()`,执行对应的函数
  - `google::protobuf::controller`：RpcController，相当于一个配置文件
  - `google::protobuf::Closure`: 执行的回调函数, 好像并没有用上

## 面试的

- IO多路复用是什么？
  - 以前处理请求的时候，客户端可以开一个进程，或者说开一个线程去处理他，但是这样也太浪费时间了，因为开辟进程和线程的时间消费是非常高的，（上下文切换啥的，两毫秒那样的时间复杂度），于是就产生了IO多路复用。使用一个进程来维护多个socket()，这样就可以在一个进程中处理很多的IO事件
- select/poll/epoll的相关考点
  - select
    - fd集合是一个数组，每次使用都会将其从用户态拷贝到内核态，耗费时间
    - 数组需要便利，浪费时间
    - 文件描述符是1024,不够大
  - poll
    - poll其实就是突破了文件描述符的最大限制,并且固定大小的集合换成了由链表实现的动态数组。但是还是受文件描述符的限制。
  - epoll
    - 将用户关心的文件描述符事件存放在内核中，通过内存映射，在用户态进行访问，（重复拷贝是利用mmap这样的技术来实现避免拷贝
    - 将事件（回调函数）加入到就绪列表中，调用`epoll_wait`就可以检查是否有多于的fd,然后给用户就可以了
    - 工作模式：
      - 水平触发（默认）：当有刻度事件的时候，就会一直epoll_wait()等待数据的出现，直到内核缓冲区数据被read()完之后才能使用。
      - 边缘触发：当有可读事件触发的时候，就会从epoll_wait中只wait()一次，告诉有数据来

### 工作模式

- LT模式：默认的工作模式，检测触发的时候，不会立刻处理事件，放到就绪列表中，下次使用，再通知
- ET模式：必须要处理，下次调用的时候，不会再次相应（只支持非堵塞模式），

## 项目疑问

- 主要实现的功能？
  - 一个是
- IO多路复用于线程池的池化思想有什么共同和特殊之处？为什么不用池化的思想去处理数据
  - 普通的线程池不能这样处理，因为客户端的请求可能会来很多这样的请求，比如说成百上千个，池化的思想虽然说里面有线程池，但是远远不够这些数字，所以说只能用IO多路复用，`IO多路复用一次性可以处理成百上千的任务，但是线程池可执行不了这些任务，会堵塞在那里`。
- Muti-Reactor的架构的优势是什么？为什么选用这个架构？
  - 传统的线程模型都是堵塞的类型，也就是说，当一个线程去处理一个IO事件的时候，就会堵塞在这里，发生堵塞的话，就没有办法处理其他的事务了，那么多个任务来到这里时候，就会堵在这里，没有更好的办法处理任务，
  - 那么对于Reactor架构呢，他是非堵塞的，主线程是一个EvnetLoop循环，还有四个副线程，也就是subReactor，当有任务监听的时候，主线程将这个文件描述符`Dispatcher`一下，将这些文件描述符给subReactor进行处理，所以说主线程是非堵塞的，异步的，然后subReactor处理之后，将结果发送给客户端，除了客户端需要等待这个结果，其他的都是异步执行的。
- 项目中实现了什么基本内容？
  - 是一个非常简易的Reactor架构的rpc框架，好像确实比较简单。。主要实现的基本内容有：
    - Reactor架构中十分重要的主从EventLoop的IO多路复用循环体系结构
    - 为了在网络上进行传输，实现了自定义协议包，然后使用protobuf进行序列化
    - 实现了一个简单的异步日志进行打印日志的功能
- 日志的处理？竞争状态？
  - 每一个线程中会将日志输送到一个共同的buffer里面，这时候因为涉及到竞争状态，所以就需要互斥锁去锁住这个资源，防止出现数据竞争
  - 之后，会将一个定时任务推送到EventLoop循环中进行执行，这时候，每隔一段事件去判断判断将这个logger里面的数据输送到AsyncLogger的loop循环中，然后将日志打印到本地，方便调试。这里相当于一个`生产者-消费者的模型`。
- 为什么要设计自定义协议？为什么采用protobuf这样的序列化的东西？可以不序列化后传输到网络上，然后直接读取么？
  - 不行的，因为 网络序和字节序是不一样的，网络序是大端，然后字节序是小端，你如果直接传输，会相反，所以要进行序列化，然后变成网络序，然后再变成字节序
  - 网络上传输的都是字节，但是本地的是得到的结果是字符串，那么你就需要序列化成字节，然后再在网络上进行传播，
  - Tcp设置协议的时候不会说有粘包这一说，因为字节流，Tcp只是保证了你会传送到这个端口上，但是并不保证了数据是怎么字节的划分，这时候就需要设计一个自定义协议，提供给客户端和服务端进行接卸
- 协议的大致样子是什么呢？
  - 开始符 - 整包长度 - MsigID长度 - MsgID - 方法名长度 - 方法名 - 错误码 - 错误信息长度 - 错误信息 - protobuf序列化数据 - 校验码 - 结束符
  - 设计校验码的问题是：`Tcp只是保证了数据按照顺序到达你这个服务端，但是并没有保证数据可能出现泄密篡改，所以说还要进行安全校验`。
- 协议是如何进行可扩展的？
  - 协议是设计成虚函数的样子进行扩展的，最上面的是函数是虚函数，然后下面是实现的函数，`注意：这里的有一个专门的智能指针进行转换`
- 

## 项目代码构建

- 项目的基本流程：
  - 从 buffer里面读取数据，然后解码得到对应的protobuf对象，然后解析相关的内容
  - 找到对应的`request type` 和 `response type` 请求类型和返回的类型
  - 执行相关的函数
  - 将reponse对象序列化成pb_data，然后将其塞入到结构体中，然后将其encode,序列化之后发送到客户端手中
- 项目的大致流程
  - 客户端
  - 服务端

## 项目遇到的问题

- 